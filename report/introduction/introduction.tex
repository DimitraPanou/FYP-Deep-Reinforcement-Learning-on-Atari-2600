\chapter{Introduction}

\section{Motivation}
Machine Learning (ML) and AI in 2018 are subjects that are almost unique in their ability to permeate into nearly every sphere, community and space in today's society. From the research community to the business world and the public eye through extensive media coverage, ML is certainly becoming more and more of a de facto part of our everyday lives. Businesses employ recommender systems to suggest new products to their customers and predict the rise and fall of stock prices using function approximators like Deep Learning. Traditional home appliances are now outdated in favour of smarter, IoT systems that learn our habits and provide a more tailored experience. \paragraph{}

ML is a broad umbrella term, encapsulating a variety of different approaches. Most ML tasks can be classified as either supervised or unsupervised learning. Deep Learning is fast becoming a popular and powerful technique in supervised learning, involving teaching artificial neural networks to approximate any function, given enough training data. RL is another branch of ML that perhaps receives less public attention but is nonetheless believed to be set to revolutionize the field of AI (\citet{survey-drl}). Recent breakthroughs in the application of Deep Learning to RL algorithms has spawned the exciting research field of DRL which has produced to date unparalleled results in various AI domains, such as defeating the world champion Go player (\citet{44806}). \paragraph{}

There are a growing number of RL methods and algorithms, such as Monte-Carlo, Q-Learning, SARSA and Policy Search (\citet{sutton1998reinforcement}). More recently, the advent of DRL has brought about adaptations to existing algorithms to expand their use to multi-dimensional observation spaces such as pixel information, a notable example being Deep Q-Learning (\citet{deepmind1}). \paragraph{}

\section{Objectives}
The overall objectives of this project are threefold. 

\begin{enumerate}
	\item Investigate the current state of the art of RL for video games.
	\item Build a system to evaluate the performance of three state of the art DRL algorithms by collecting a series of metrics while applying each algorithm to a selection of Atari 2600 video games. 
	\item Compare and contrast the three algorithms by performing evaluation experiments. Investigate the game scores, survival times and model losses
\end{enumerate}

In Chapter \ref{ch:design} we outline a set of design and experimentation goals to facilitate the overall project goals. The system is given no prior knowledge of how each game works and there is no change in the underlying architecture of the solution when applied to different games, all while maintaining a high level of performance. The aim for the system is to be a general solution, that it can be expanded to work for any number of games and algorithms in the future with ease of implementation. The algorithms used are DQN, 2DQN and 3DQN. These algorithms were developed by DeepMind and published in (\citet{deepmind1,doubleq,dueling}) respectively. They were chosen for their well known performance in the domain of Atari 2600 video games and the natural progression that follows from one to the next.

\section{Research Methods}
This project takes a case study based approach to the experimentation. The first phase of the project involves building the system to the specification outlined previously. The second phase treats each game entered into the system as an individual case study. The game ROM is given as input to the system. The game is simulated by a third party emulator of our choosing, discussed in Chapter \ref{ch:design}, from which the system extracts greyscale frames to learn from. The output of the system is an action that it has chosen to be optimal, selected from the discrete vector of possible actions as defined by the game's control scheme. This control scheme is not provided to the system, it determines it dynamically with each game. The action is fed back into the emulator and the cycle continues up to a terminating signal.

\section{Report Overview}
\textbf{Chapter 2} gives some necessary background information. It will introduce the fields of ML and RL, the seminal Deep Q-Network and the technologies and tools being used in video game RL research today.

\textbf{Chapter 3} discusses the current state of the art of RL, with particular interest to the fields of robotics, natural language processing and video games.

\textbf{Chapter 4} outlines the architecture of the system and the rationale behind certain design decisions.

\textbf{Chapter 5} will discuss the components of the evaluation experiments. It will give a description of the experimental setup, and a discussion of the results.

\textbf{Chapter 6} closes the project with a conclusion of all that has been discussed, an outline of what has been achieved from both an objective and personal point of view and finally a suggestion for future work.
